# -*- coding: utf-8 -*-
"""a10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12z1IZa_UA0Q8lKH2Kvf6DORx_3vSqFFX

Calculate Term Frequency and Inverse Document Frequency. Considering sentences of documents.
"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import math

# Sample document containing multiple sentences
document = """ Natural language processing (NLP) is a field of artificial intelligence concerned with the interaction between computers and humans in natural language. """

# Tokenize the document into sentences
sentences = sent_tokenize(document)

# Tokenize each sentence into words and remove stop words
stop_words = set(stopwords.words('english'))

tokens = [word_tokenize(sentence.lower()) for sentence in sentences]
tokens_without_stopwords = [[word for word in words if word not in stop_words] for words in tokens]

# Calculate Term Frequency (TF) for each sentence
tf_sentences = [{term: count/len(sentence) for term, count in Counter(words).items()} for words, sentence in zip(tokens_without_stopwords, tokens)]

# Calculate Inverse Document Frequency (IDF) for each term
all_terms = set([term for words in tokens_without_stopwords for term in words])
idf = {term: math.log(len(sentences) / sum([1 for words in tokens_without_stopwords if term in words])) for term in all_terms}

# Print TF and IDF for each sentence and term
print("Term Frequency (TF) for each sentence:")
for i, tf_sentence in enumerate(tf_sentences, 1):
    print(f"Sentence {i}: {tf_sentence}")

print("\nInverse Document Frequency (IDF) for each term:")
for term, idf_value in idf.items():
    print(f"Term '{term}': IDF = {idf_value:.4f}")

"""Tokenizes the document into sentences using NLTK's sent_tokenize function. Tokenizes each sentence into words, converts them to lowercase, and removes stop words. Calculates Term Frequency (TF) for each sentence by counting the frequency of each term and dividing it by the total number of terms in the sentence. Calculates Inverse Document Frequency (IDF) for each term by counting the number of documents containing each term and taking the logarithm of the ratio of the total number of documents to the number of documents containing the term. Prints the TF and IDF values for each sentence and term."""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from collections import Counter
import math

# Read document from file
def read_document(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

# Define file path
file_path = 'p10.txt'

# Tokenize document into sentences
document = read_document(file_path)
sentences = sent_tokenize(document)

# Tokenize sentences into words and remove stop words
stop_words = set(stopwords.words('english'))
tokens = [word_tokenize(sentence.lower()) for sentence in sentences]
tokens_without_stopwords = [[word for word in words if word not in stop_words] for words in tokens]

# Calculate Term Frequency (TF) for each sentence
tf_sentences = [{term: count/len(sentence) for term, count in Counter(words).items()} for words, sentence in zip(tokens_without_stopwords, tokens)]

# Calculate Inverse Document Frequency (IDF) for each term
all_terms = set([term for words in tokens_without_stopwords for term in words])
idf = {term: math.log(len(sentences) / sum([1 for words in tokens_without_stopwords if term in words])) for term in all_terms}

# Print TF and IDF for each sentence and term
print("Term Frequency (TF) for each sentence:")
for i, tf_sentence in enumerate(tf_sentences, 1):
    print(f"Sentence {i}: {tf_sentence}")

print("\nInverse Document Frequency (IDF) for each term:")
for term, idf_value in idf.items():
    print(f"Term '{term}': IDF = {idf_value:.4f}")

"""Term Frequency (TF) and Inverse Document Frequency (IDF) are two important concepts in natural language processing (NLP) used for text analysis and information retrieval tasks.

1. **Term Frequency (TF)**:
   - Term Frequency measures the frequency of a term (word) in a document relative to the total number of terms in that document.
   - It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document.
   - Mathematically, TF is calculated using the formula:
     \[ \text{TF}(t, d) = \frac{\text{Number of times term \( t \) appears in document \( d \)}}{\text{Total number of terms in document \( d \)}} \]
   - TF values are normalized to prevent bias towards longer documents.

2. **Inverse Document Frequency (IDF)**:
   - Inverse Document Frequency measures the rarity of a term across all documents in a corpus.
   - It is calculated as the logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the term, scaled to avoid division by zero.
   - Mathematically, IDF is calculated using the formula:
     \[ \text{IDF}(t, D) = \log\left( \frac{\text{Total number of documents in corpus \( D \)}}{\text{Number of documents containing term \( t \) in corpus \( D \)}} \right) \]
   - IDF values increase with the rarity of a term across documents.

Once TF and IDF values are calculated, they are often combined to form TF-IDF (Term Frequency-Inverse Document Frequency), which is a weighting scheme used to evaluate the importance of a term in a document relative to a corpus.

Here's how you can calculate TF and IDF for sentences in a document:

1. Tokenize the sentences and count the frequency of each term (word) within each sentence to calculate TF.
2. Count the number of documents containing each term to calculate IDF.
3. Combine TF and IDF to calculate TF-IDF.

If you need code examples for implementing TF and IDF calculations in Python, let me know!
"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import math

# Sample document containing multiple sentences
document = """
            Natural language processing (NLP) is a field of artificial intelligence
            concerned with the interaction between computers and humans in natural
            language. It focuses on the interaction between computers and humans
            through natural language processing. NLP techniques are used to analyze,
            understand, and generate human language in a valuable way.
            """

# Tokenize the document into sentences
sentences = sent_tokenize(document)

# Tokenize each sentence into words and remove stop words
stop_words = set(stopwords.words('english'))
tokens = [word_tokenize(sentence.lower()) for sentence in sentences]
tokens_without_stopwords = [[word for word in words if word not in stop_words] for words in tokens]

# Calculate Term Frequency (TF) for each sentence
tf_sentences = [{term: count/len(sentence) for term, count in Counter(words).items()} for words, sentence in zip(tokens_without_stopwords, tokens)]

# Calculate Inverse Document Frequency (IDF) for each term
all_terms = set([term for words in tokens_without_stopwords for term in words])
idf = {term: math.log(len(sentences) / sum([1 for words in tokens_without_stopwords if term in words])) for term in all_terms}

# Print TF and IDF for each sentence and term
print("Term Frequency (TF) for each sentence:")
for i, tf_sentence in enumerate(tf_sentences, 1):
    print(f"Sentence {i}: {tf_sentence}")

print("\nInverse Document Frequency (IDF) for each term:")
for term, idf_value in idf.items():
    print(f"Term '{term}': IDF = {idf_value:.4f}")

