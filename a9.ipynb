{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For given text apply following preprocessing methods:\n",
    "1. Tokenization\n",
    "2. POS Tagging\n",
    "3. Stop word Removal\n",
    "4. Lemmatization\n",
    "5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\purva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Text tokenization serves as the initial phase of natural languages processing. It involves segmenting a given text into individual units, like words or sentences, which are known as tokens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: ['Text', 'tokenization', 'serves', 'as', 'the', 'initial', 'phase', 'of', 'natural', 'languages', 'processing', '.', 'It', 'involves', 'segmenting', 'a', 'given', 'text', 'into', 'individual', 'units', ',', 'like', 'words', 'or', 'sentences', ',', 'which', 'are', 'known', 'as', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokenization:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging: [('Text', 'NNP'), ('tokenization', 'NN'), ('serves', 'VBZ'), ('as', 'IN'), ('the', 'DT'), ('initial', 'JJ'), ('phase', 'NN'), ('of', 'IN'), ('natural', 'JJ'), ('languages', 'NNS'), ('processing', 'VBG'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('segmenting', 'VBG'), ('a', 'DT'), ('given', 'VBN'), ('text', 'NN'), ('into', 'IN'), ('individual', 'JJ'), ('units', 'NNS'), (',', ','), ('like', 'IN'), ('words', 'NNS'), ('or', 'CC'), ('sentences', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('known', 'VBN'), ('as', 'IN'), ('tokens', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tagging:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stop word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words:  {'nor', 'below', 'your', 'my', 'during', \"didn't\", 'on', 'his', 'itself', \"couldn't\", 'these', 'myself', 'me', 'at', 'about', 'ain', 'hasn', 'own', 'will', 'wouldn', 'doesn', \"hasn't\", 've', \"you're\", 'any', 'up', 'how', 'further', 'you', 'wasn', 'have', 'than', \"doesn't\", 'where', 'in', 'that', 'no', 'with', \"weren't\", 'had', 'out', 'such', 'haven', 'shouldn', 'between', 'should', 'i', 'it', 'because', 'just', 'now', 'or', 'couldn', 'few', 'other', 'mightn', 'herself', 'having', 'll', 'while', \"won't\", 'to', 'they', 'over', 'did', 'both', 'most', 'we', 'ourselves', 'isn', 'shan', 'were', 'above', \"should've\", 'who', 'very', 'didn', \"hadn't\", 'of', 'from', 'o', 'so', 'again', 'aren', 'has', 'won', 'this', \"shouldn't\", 'weren', 'for', 'then', 'does', 'them', \"mightn't\", \"isn't\", 'yourselves', 'and', 'those', 'don', 'mustn', 'him', 'needn', \"haven't\", 'y', 'off', \"that'll\", 'are', 'against', 'theirs', \"needn't\", \"she's\", 'some', 'there', 'all', 'a', 'her', 'same', 'm', 'ma', \"shan't\", 'doing', 'once', \"you've\", 'after', 'down', 'our', 'yours', \"aren't\", 'himself', 'their', 'here', 'if', 'before', 'as', 't', 'hadn', 'can', 'whom', 'into', 'too', \"don't\", \"wouldn't\", 're', \"wasn't\", 'an', 's', \"mustn't\", 'what', 'am', \"you'd\", 'yourself', 'why', 'she', 'through', 'more', 'themselves', 'd', 'each', 'is', 'was', 'ours', 'its', 'which', 'being', 'not', \"it's\", 'by', 'he', 'hers', 'but', 'only', 'do', 'until', 'be', 'under', 'been', \"you'll\", 'the', 'when'}\n",
      "Stop word Removal: ['Text', 'tokenization', 'serves', 'initial', 'phase', 'natural', 'languages', 'processing', '.', 'involves', 'segmenting', 'given', 'text', 'individual', 'units', ',', 'like', 'words', 'sentences', ',', 'known', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"stop_words: \",stop_words)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Stop word Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['Text', 'tokenization', 'serf', 'initial', 'phase', 'natural', 'language', 'processing', '.', 'involves', 'segmenting', 'given', 'text', 'individual', 'unit', ',', 'like', 'word', 'sentence', ',', 'known', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatization:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['text', 'token', 'serv', 'initi', 'phase', 'natur', 'languag', 'process', '.', 'involv', 'segment', 'given', 'text', 'individu', 'unit', ',', 'like', 'word', 'sentenc', ',', 'known', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemming:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Tokenization**:\n",
    "   - Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, or other meaningful elements.\n",
    "   - For example, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" Tokenization would split this sentence into individual words: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"].\n",
    "   - Tokenization is a crucial step in natural language processing (NLP) tasks as it helps in extracting meaningful information from text data.\n",
    "\n",
    "2. **POS Tagging** (Part-of-Speech Tagging):\n",
    "   - POS tagging is the process of assigning a part-of-speech tag (such as noun, verb, adjective, etc.) to each word in a sentence.\n",
    "   - This helps in understanding the grammatical structure of a sentence and aids in various NLP tasks such as named entity recognition, text summarization, etc.\n",
    "   - For example, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" POS tagging would assign tags to each word: [(\"The\", \"DT\"), (\"quick\", \"JJ\"), (\"brown\", \"JJ\"), (\"fox\", \"NN\"), (\"jumps\", \"VBZ\"), (\"over\", \"IN\"), (\"the\", \"DT\"), (\"lazy\", \"JJ\"), (\"dog\", \"NN\"), (\".\", \".\")].\n",
    "   - Here, \"DT\" represents determiner, \"JJ\" represents adjective, \"NN\" represents noun, \"VBZ\" represents verb (3rd person singular present), and \"IN\" represents preposition or conjunction.\n",
    "\n",
    "3. **Stop Word Removal**:\n",
    "   - Stop words are common words that are often considered irrelevant for text analysis as they do not carry much meaning (e.g., \"the\", \"is\", \"are\", \"and\", \"but\", etc.).\n",
    "   - Stop word removal is the process of filtering out these stop words from text data.\n",
    "   - This helps in reducing noise in the data and improving the efficiency of text processing algorithms.\n",
    "   - For example, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" After stop word removal, it would become: [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"].\n",
    "\n",
    "4. **Lemmatization**:\n",
    "   - Lemmatization is the process of reducing words to their base or dictionary form (known as lemma) while still ensuring that the reduced form belongs to the language.\n",
    "   - It involves removing inflections and variations to bring words to their root form.\n",
    "   - For example, the lemma of the words \"running\", \"ran\", and \"runs\" is \"run\".\n",
    "   - Lemmatization helps in standardizing words so that variations of the same word are treated as the same entity in text analysis.\n",
    "\n",
    "5. **Stemming**:\n",
    "   - Stemming is similar to lemmatization, but it is a more crude and rule-based approach.\n",
    "   - It involves removing prefixes and suffixes from words to reduce them to their root or stem form.\n",
    "   - Stemming is more aggressive than lemmatization and may not always result in valid words.\n",
    "   - For example, stemming the words \"running\", \"ran\", and \"runs\" would result in \"run\".\n",
    "   - Stemming is computationally less expensive compared to lemmatization and is often used in information retrieval systems and text mining tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
